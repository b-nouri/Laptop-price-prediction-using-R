View(clean6)
View(clean_test3)
maxPrice_Clean_Training <- clean6 %>%
select(brand_mean_max,base_name_mean_max,touchscreen,screen_surface,screen_size,weight,ram,storage,ssd,resolution_mean_max,discrete_gpu,gpu_mean_max,cpu_mean_max,display_mean_max,x360,os, max_price)
#maxPrice_Clean_Training <- data.frame(model.matrix(~., data=maxPrice_Clean_Training_prev))
minPrice_Clean_Training <- clean6 %>%
select(brand_mean_max,base_name_mean_max,touchscreen,screen_surface,screen_size,weight,ram,storage,ssd,resolution_mean_max,discrete_gpu,gpu_mean_max,cpu_mean_max,display_mean_max,x360,os, min_price)
#minPrice_Clean_Training <- data.frame(m
set.seed(123) #For reproducibility
train.control <- trainControl(method = "repeatedcv",
number = 20, repeats = 3)
##### Train the model 7 Parallel Random Forest  <---------------BEST MODEL SO FAR
model7_max <- train(max_price ~ . , data = maxPrice_Clean_Training,
method = "parRF", trControl = train.control, metric = "MAE")
##### Train the model 7 Parallel Random Forest  <---------------BEST MODEL SO FAR
model7_min <- train(min_price ~ . , data = minPrice_Clean_Training,
method = "parRF", trControl = train.control, metric = "MAE")
print(min(model7_max$results$MAE+model7_min$results$MAE))
maxPrice_Clean_Training <- clean6 %>%
select(brand_mean_max,base_name_mean_max,touchscreen,screen_surface,screen_size_fact,weight,ram,storage,ssd,resolution_mean_max,discrete_gpu,gpu_mean_max,cpu_mean_max,display_mean_max,x360,os, max_price)
#maxPrice_Clean_Training <- data.frame(model.matrix(~., data=maxPrice_Clean_Training_prev))
minPrice_Clean_Training <- clean6 %>%
select(brand_mean_max,base_name_mean_max,touchscreen,screen_surface,screen_size_fact,weight,ram,storage,ssd,resolution_mean_max,discrete_gpu,gpu_mean_max,cpu_mean_max,display_mean_max,x360,os, min_price)
#minPrice_Clean_Training <- data.frame(model.matrix(~., data=minPrice_Clean_Training_prev))
set.seed(123) #For reproducibility
train.control <- trainControl(method = "repeatedcv",
number = 20, repeats = 3)
##### Train the model 7 Parallel Random Forest  <---------------BEST MODEL SO FAR
model7_max <- train(max_price ~ . , data = maxPrice_Clean_Training,
method = "parRF", trControl = train.control, metric = "MAE")
##### Train the model 7 Parallel Random Forest  <---------------BEST MODEL SO FAR
model7_min <- train(min_price ~ . , data = minPrice_Clean_Training,
method = "parRF", trControl = train.control, metric = "MAE")
print(min(model7_max$results$MAE+model7_min$results$MAE))
maxPrice_Clean_Training <- clean6 %>%
select(brand_mean_max,base_name_mean_max,touchscreen,screen_surface,screen_size,weight,ram,storage,ssd,resolution_mean_max,discrete_gpu,gpu_mean_max,cpu_mean_max,display_mean_max,x360,os, max_price)
#maxPrice_Clean_Training <- data.frame(model.matrix(~., data=maxPrice_Clean_Training_prev))
minPrice_Clean_Training <- clean6 %>%
select(brand_mean_min,base_name_mean_min,touchscreen,screen_surface,screen_size,weight,ram,storage,ssd,resolution_mean_min,discrete_gpu,gpu_mean_min,cpu_mean_min,display_mean_min,x360,os, min_price)
#minPrice_Clean_Training <- data.frame(model.matrix(~., data=minPrice_Clean_Training_prev))
# Training control definition
set.seed(123) #For reproducibility
train.control <- trainControl(method = "repeatedcv",
number = 20, repeats = 3)
#---- Best model (Parallel Random Forest)--------------------
##### Train the model 7 Parallel Random Forest  <---------------BEST MODEL SO FAR
model7_max <- train(max_price ~ . , data = maxPrice_Clean_Training,
method = "parRF", trControl = train.control, metric = "MAE")
##### Train the model 7 Parallel Random Forest  <---------------BEST MODEL SO FAR
model7_min <- train(min_price ~ . , data = minPrice_Clean_Training,
method = "parRF", trControl = train.control, metric = "MAE")
print(min(model7_max$results$MAE+model7_min$results$MAE))
maxPrice_Clean_Training <- clean6 %>%
select(brand_mean_max,base_name_mean_max,touchscreen,screen_surface,screen_size,weight_clean,ram,storage,ssd,resolution_mean_max,discrete_gpu,gpu_mean_max,cpu_mean_max,display_mean_max,x360,os, max_price)
#maxPrice_Clean_Training <- data.frame(model.matrix(~., data=maxPrice_Clean_Training_prev))
minPrice_Clean_Training <- clean6 %>%
select(brand_mean_min,base_name_mean_min,touchscreen,screen_surface,screen_size,weight_clean,ram,storage,ssd,resolution_mean_min,discrete_gpu,gpu_mean_min,cpu_mean_min,display_mean_min,x360,os, min_price)
#minPrice_Clean_Training <- data.frame(model.matrix(~., data=minPrice_Clean_Training_prev))
#
# Training control definition
set.seed(123) #For reproducibility
train.control <- trainControl(method = "repeatedcv",
number = 20, repeats = 3)
#---- Best model (Parallel Random Forest)--------------------
##### Train the model 7 Parallel Random Forest  <---------------BEST MODEL SO FAR
model7_max <- train(max_price ~ . , data = maxPrice_Clean_Training,
method = "parRF", trControl = train.control, metric = "MAE")
##### Train the model 7 Parallel Random Forest  <---------------BEST MODEL SO FAR
model7_min <- train(min_price ~ . , data = minPrice_Clean_Training,
method = "parRF", trControl = train.control, metric = "MAE")
print(min(model7_max$results$MAE+model7_min$results$MAE))
maxPrice_Clean_Training <- clean6 %>%
select(brand_mean_max,base_name_mean_max,touchscreen,screen_surface,screen_size,weight,ram,storage,ssd,resolution_mean_max,discrete_gpu,gpu_mean_max,cpu_mean_max,display_mean_max,x360,os, max_price)
#maxPrice_Clean_Training <- data.frame(model.matrix(~., data=maxPrice_Clean_Training_prev))
minPrice_Clean_Training <- clean6 %>%
select(brand_mean_min,base_name_mean_min,touchscreen,screen_surface,screen_size,weight,ram,storage,ssd,resolution_mean_min,discrete_gpu,gpu_mean_min,cpu_mean_min,display_mean_min,x360,os, min_price)
#minPrice_Clean_Training <- data.frame(model.matrix(~., data=minPrice_Clean_Training_prev))
# Training control definition
set.seed(123) #For reproducibility
train.control <- trainControl(method = "repeatedcv",
number = 20, repeats = 3)
#--------Models for maxPrice with Normalized data (except decision tree models) -----------------
#Features: brand, touchscreen, screen_size , weight, ram, storage, ssd, resolution, discrete_gpu,
#          cpu_benchmark_score, gpu_benchmark_score
##### Train the model 1 (Linear regression)
model1_max <- train(max_price ~ . , data = maxPrice_Norm_Training,
method = "lm", trControl = train.control, metric = "MAE") #warning a lot of features
##### Train the model 2 (Generalized Linear Model without func specified -> could be improved)
model2_max <- train(max_price ~ . , data = maxPrice_Norm_Training,
method = "glm", trControl = train.control, metric = "MAE") #warning a lot of features
##### Train the model 3 (Bayesian GLM)
model3_max <- train(max_price ~ . , data = maxPrice_Norm_Training,
method = "bayesglm", trControl = train.control, metric = "MAE")
##### Train the model 4 (Elastic net (glm))
model4_max <- train(max_price ~ . , data = maxPrice_Norm_Training,
method = "glmnet", trControl = train.control, metric = "MAE")
##### Train the model 5 Boosted Tree
model5_max <- train(max_price ~ . , data = maxPrice_Clean_Training,
method = "bstTree", trControl = train.control, metric = "MAE")
##### Train the model 6 eXtreme Gradient Boosting
model6_max <- train(max_price ~ . , data = maxPrice_Clean_Training,
method = "xgbTree", trControl = train.control, metric = "MAE")
##### Train the model 7 Parallel Random Forest  <---------------BEST MODEL SO FAR
model7_max <- train(max_price ~ . , data = maxPrice_Clean_Training,
method = "parRF", trControl = train.control, metric = "MAE")
##### Train the model 8 Stochastic Gradient Boosting # warning for some brands (few observations)
model8_max <- train(max_price ~ . , data = maxPrice_Clean_Training,
method = "gbm", trControl = train.control, metric = "MAE")
#--------Models for min_price with Normalized data (except decision tree models) -----------------
#Features: brand, touchscreen, screen_size , weight, ram, storage, ssd, resolution, discrete_gpu,
#          cpu_benchmark_score, gpu_benchmark_score
##### Train the model 1 (Linear regression)
model1_min <- train(min_price ~ . , data = minPrice_Norm_Training,
method = "lm", trControl = train.control, metric = "MAE") #warning a lot of features
##### Train the model 2 (Generalized Linear Model without func specified -> could be improved)
model2_min <- train(min_price ~ . , data = minPrice_Norm_Training,
method = "glm", trControl = train.control, metric = "MAE") #warning a lot of features
##### Train the model 3 (GLM with Step AIC)
model3_min <- train(min_price ~ . , data = minPrice_Norm_Training,
method = "bayesglm", trControl = train.control, metric = "MAE")
##### Train the model 4 (Elastic net (glm))
model4_min <- train(min_price ~ . , data = minPrice_Norm_Training,
method = "glmnet", trControl = train.control, metric = "MAE")
##### Train the model 5 Boosted Tree
model5_min <- train(min_price ~ . , data = minPrice_Clean_Training,
method = "bstTree", trControl = train.control, metric = "MAE")
##### Train the model 6 eXtreme Gradient Boosting
model6_min <- train(min_price ~ . , data = minPrice_Clean_Training,
method = "xgbTree", trControl = train.control, metric = "MAE")
##### Train the model 7 Parallel Random Forest  <---------------BEST MODEL SO FAR
model7_min <- train(min_price ~ . , data = minPrice_Clean_Training,
method = "parRF", trControl = train.control, metric = "MAE")
##### Train the model 8 Stochastic Gradient Boosting # warning for some brands (few observations)
model8_min <- train(min_price ~ . , data = minPrice_Clean_Training,
method = "gbm", trControl = train.control, metric = "MAE")
#------- Summarize the results ----------------
#Features: brand, touchscreen, screen_size , weight, ram, storage, ssd, resolution, discrete_gpu,
#          cpu_benchmark_score, gpu_benchmark_score
print(model1_max$results$MAE+model1_min$results$MAE)
print(model2_max$results$MAE+model2_min$results$MAE)
print(model3_max$results$MAE+model3_min$results$MAE)
print(min(model4_max$results$MAE+model4_min$results$MAE))
print(min(model5_max$results$MAE+model5_min$results$MAE))
print(min(model6_max$results$MAE+model6_min$results$MAE))
print(min(model7_max$results$MAE+model7_min$results$MAE)) # <---------------BEST MODEL SO FAR
print(min(model8_max$results$MAE+model8_min$results$MAE))
maxPrice_Clean_Training <- clean6 %>%
select(brand_mean_max,base_name_mean_max,touchscreen,screen_surface,screen_size,weight,ram,storage,ssd,resolution_mean_max,discrete_gpu,gpu_mean_max,cpu_mean_max,display_mean_max,x360,os, max_price)
#maxPrice_Clean_Training <- data.frame(model.matrix(~., data=maxPrice_Clean_Training_prev))
minPrice_Clean_Training <- clean6 %>%
select(brand_mean_min,base_name_mean_min,touchscreen,screen_surface,screen_size,weight,ram,storage,ssd,resolution_mean_min,discrete_gpu,gpu_mean_min,cpu_mean_min,display_mean_min,x360,os, min_price)
#minPrice_Clean_Training <- data.frame(model.matrix(~., data=minPrice_Clean_Training_prev))
#
#
# #-------- Data normalization -------------------
index_Response <- match(c("max_price", "min_price"), names(clean6))
preProcValues <- preProcess(clean6[-index_Response], method = "range")
trainScaled <- predict(preProcValues, clean6)
glimpse(trainScaled)
testScaled <- predict(preProcValues, clean_test3)
glimpse(testScaled)
#
# # Selecting only the features to use for Normalized data
maxPrice_Norm_Training_prev <- trainScaled %>%
select(brand_mean_max,base_name_mean_max,touchscreen,screen_surface,screen_size,weight,ram,storage,ssd,resolution_mean_max,discrete_gpu,gpu_mean_max,cpu_mean_max,display_mean_max,x360,os, max_price)
# maxPrice_Norm_Training <- data.frame(model.matrix(~., data=maxPrice_Norm_Training_prev))
# maxPrice_Norm_Training
#
minPrice_Norm_Training_prev <- trainScaled %>%
select(brand_mean_min,base_name_mean_min,touchscreen,screen_surface,screen_size,weight,ram,storage,ssd,resolution_mean_min,discrete_gpu,gpu_mean_min,cpu_mean_min,display_mean_min,x360,os, min_price)
# minPrice_Norm_Training <- data.frame(model.matrix(~., data=minPrice_Norm_Training_prev))
maxPrice_Clean_Training <- clean6 %>%
select(brand_mean_max,base_name_mean_max,touchscreen,screen_surface,screen_size,weight,ram,storage,ssd,resolution_mean_max,discrete_gpu,gpu_mean_max,cpu_mean_max,display_mean_max,x360,os, max_price)
#maxPrice_Clean_Training <- data.frame(model.matrix(~., data=maxPrice_Clean_Training_prev))
minPrice_Clean_Training <- clean6 %>%
select(brand_mean_min,base_name_mean_min,touchscreen,screen_surface,screen_size,weight,ram,storage,ssd,resolution_mean_min,discrete_gpu,gpu_mean_min,cpu_mean_min,display_mean_min,x360,os, min_price)
#minPrice_Clean_Training <- data.frame(model.matrix(~., data=minPrice_Clean_Training_prev))
#
#
# #-------- Data normalization -------------------
index_Response <- match(c("max_price", "min_price"), names(clean6))
preProcValues <- preProcess(clean6[-index_Response], method = "range")
trainScaled <- predict(preProcValues, clean6)
glimpse(trainScaled)
testScaled <- predict(preProcValues, clean_test3)
glimpse(testScaled)
#
# # Selecting only the features to use for Normalized data
maxPrice_Norm_Training <- trainScaled %>%
select(brand_mean_max,base_name_mean_max,touchscreen,screen_surface,screen_size,weight,ram,storage,ssd,resolution_mean_max,discrete_gpu,gpu_mean_max,cpu_mean_max,display_mean_max,x360,os, max_price)
# maxPrice_Norm_Training <- data.frame(model.matrix(~., data=maxPrice_Norm_Training_prev))
# maxPrice_Norm_Training
#
minPrice_Norm_Training <- trainScaled %>%
select(brand_mean_min,base_name_mean_min,touchscreen,screen_surface,screen_size,weight,ram,storage,ssd,resolution_mean_min,discrete_gpu,gpu_mean_min,cpu_mean_min,display_mean_min,x360,os, min_price)
# minPrice_Norm_Training <- data.frame(model.matrix(~., data=minPrice_Norm_Training_prev))
#
#
#
#------Repeated K-Fold Cross Validation (K = 20, repeats = 3)----------------
# Training control definition
set.seed(123) #For reproducibility
train.control <- trainControl(method = "repeatedcv",
number = 20, repeats = 3)
#--------Models for maxPrice with Normalized data (except decision tree models) -----------------
#Features: brand, touchscreen, screen_size , weight, ram, storage, ssd, resolution, discrete_gpu,
#          cpu_benchmark_score, gpu_benchmark_score
##### Train the model 1 (Linear regression)
model1_max <- train(max_price ~ . , data = maxPrice_Norm_Training,
method = "lm", trControl = train.control, metric = "MAE") #warning a lot of features
##### Train the model 2 (Generalized Linear Model without func specified -> could be improved)
model2_max <- train(max_price ~ . , data = maxPrice_Norm_Training,
method = "glm", trControl = train.control, metric = "MAE") #warning a lot of features
##### Train the model 3 (Bayesian GLM)
model3_max <- train(max_price ~ . , data = maxPrice_Norm_Training,
method = "bayesglm", trControl = train.control, metric = "MAE")
##### Train the model 4 (Elastic net (glm))
model4_max <- train(max_price ~ . , data = maxPrice_Norm_Training,
method = "glmnet", trControl = train.control, metric = "MAE")
##### Train the model 5 Boosted Tree
model5_max <- train(max_price ~ . , data = maxPrice_Clean_Training,
method = "bstTree", trControl = train.control, metric = "MAE")
##### Train the model 6 eXtreme Gradient Boosting
model6_max <- train(max_price ~ . , data = maxPrice_Clean_Training,
method = "xgbTree", trControl = train.control, metric = "MAE")
##### Train the model 7 Parallel Random Forest  <---------------BEST MODEL SO FAR
model7_max <- train(max_price ~ . , data = maxPrice_Clean_Training,
method = "parRF", trControl = train.control, metric = "MAE")
##### Train the model 8 Stochastic Gradient Boosting # warning for some brands (few observations)
model8_max <- train(max_price ~ . , data = maxPrice_Clean_Training,
method = "gbm", trControl = train.control, metric = "MAE")
#--------Models for min_price with Normalized data (except decision tree models) -----------------
#Features: brand, touchscreen, screen_size , weight, ram, storage, ssd, resolution, discrete_gpu,
#          cpu_benchmark_score, gpu_benchmark_score
##### Train the model 1 (Linear regression)
model1_min <- train(min_price ~ . , data = minPrice_Norm_Training,
method = "lm", trControl = train.control, metric = "MAE") #warning a lot of features
##### Train the model 2 (Generalized Linear Model without func specified -> could be improved)
model2_min <- train(min_price ~ . , data = minPrice_Norm_Training,
method = "glm", trControl = train.control, metric = "MAE") #warning a lot of features
##### Train the model 3 (GLM with Step AIC)
model3_min <- train(min_price ~ . , data = minPrice_Norm_Training,
method = "bayesglm", trControl = train.control, metric = "MAE")
##### Train the model 4 (Elastic net (glm))
model4_min <- train(min_price ~ . , data = minPrice_Norm_Training,
method = "glmnet", trControl = train.control, metric = "MAE")
##### Train the model 5 Boosted Tree
model5_min <- train(min_price ~ . , data = minPrice_Clean_Training,
method = "bstTree", trControl = train.control, metric = "MAE")
##### Train the model 6 eXtreme Gradient Boosting
model6_min <- train(min_price ~ . , data = minPrice_Clean_Training,
method = "xgbTree", trControl = train.control, metric = "MAE")
##### Train the model 7 Parallel Random Forest  <---------------BEST MODEL SO FAR
model7_min <- train(min_price ~ . , data = minPrice_Clean_Training,
method = "parRF", trControl = train.control, metric = "MAE")
##### Train the model 8 Stochastic Gradient Boosting # warning for some brands (few observations)
model8_min <- train(min_price ~ . , data = minPrice_Clean_Training,
method = "gbm", trControl = train.control, metric = "MAE")
#------- Summarize the results ----------------
#Features: brand, touchscreen, screen_size , weight, ram, storage, ssd, resolution, discrete_gpu,
#          cpu_benchmark_score, gpu_benchmark_score
print(model1_max$results$MAE+model1_min$results$MAE)
print(model2_max$results$MAE+model2_min$results$MAE)
print(model3_max$results$MAE+model3_min$results$MAE)
print(min(model4_max$results$MAE+model4_min$results$MAE))
print(min(model5_max$results$MAE+model5_min$results$MAE))
print(min(model6_max$results$MAE+model6_min$results$MAE))
print(min(model7_max$results$MAE+model7_min$results$MAE)) # <---------------BEST MODEL SO FAR
print(min(model8_max$results$MAE+model8_min$results$MAE))
maxPrice_Clean_Training_prev <- clean6 %>%
select(brand_mean_max,base_name_mean_max,touchscreen,screen_surface,screen_size,weight,ram,storage,ssd,resolution_mean_max,discrete_gpu,gpu_mean_max,cpu_mean_max,display_mean_max,x360,os, max_price)
maxPrice_Clean_Training <- data.frame(model.matrix(~., data=maxPrice_Clean_Training_prev))
minPrice_Clean_Training_prev <- clean6 %>%
select(brand_mean_min,base_name_mean_min,touchscreen,screen_surface,screen_size,weight,ram,storage,ssd,resolution_mean_min,discrete_gpu,gpu_mean_min,cpu_mean_min,display_mean_min,x360,os, min_price)
minPrice_Clean_Training <- data.frame(model.matrix(~., data=minPrice_Clean_Training_prev))
# # Selecting only the features to use for Normalized data
maxPrice_Norm_Training_prev <- trainScaled %>%
select(brand_mean_max,base_name_mean_max,touchscreen,screen_surface,screen_size,weight,ram,storage,ssd,resolution_mean_max,discrete_gpu,gpu_mean_max,cpu_mean_max,display_mean_max,x360,os, max_price)
maxPrice_Norm_Training <- data.frame(model.matrix(~., data=maxPrice_Norm_Training_prev))
# maxPrice_Norm_Training
#
minPrice_Norm_Training_prev <- trainScaled %>%
select(brand_mean_min,base_name_mean_min,touchscreen,screen_surface,screen_size,weight,ram,storage,ssd,resolution_mean_min,discrete_gpu,gpu_mean_min,cpu_mean_min,display_mean_min,x360,os, min_price)
minPrice_Norm_Training <- data.frame(model.matrix(~., data=minPrice_Norm_Training_prev))
set.seed(123) #For reproducibility
train.control <- trainControl(method = "repeatedcv",
number = 20, repeats = 3)
#--------Models for maxPrice with Normalized data (except decision tree models) -----------------
#Features: brand, touchscreen, screen_size , weight, ram, storage, ssd, resolution, discrete_gpu,
#          cpu_benchmark_score, gpu_benchmark_score
##### Train the model 1 (Linear regression)
model1_max <- train(max_price ~ . , data = maxPrice_Norm_Training,
method = "lm", trControl = train.control, metric = "MAE") #warning a lot of features
##### Train the model 2 (Generalized Linear Model without func specified -> could be improved)
model2_max <- train(max_price ~ . , data = maxPrice_Norm_Training,
method = "glm", trControl = train.control, metric = "MAE") #warning a lot of features
##### Train the model 3 (Bayesian GLM)
model3_max <- train(max_price ~ . , data = maxPrice_Norm_Training,
method = "bayesglm", trControl = train.control, metric = "MAE")
##### Train the model 4 (Elastic net (glm))
model4_max <- train(max_price ~ . , data = maxPrice_Norm_Training,
method = "glmnet", trControl = train.control, metric = "MAE")
##### Train the model 5 Boosted Tree
model5_max <- train(max_price ~ . , data = maxPrice_Clean_Training,
method = "bstTree", trControl = train.control, metric = "MAE")
##### Train the model 6 eXtreme Gradient Boosting
model6_max <- train(max_price ~ . , data = maxPrice_Clean_Training,
method = "xgbTree", trControl = train.control, metric = "MAE")
##### Train the model 7 Parallel Random Forest  <---------------BEST MODEL SO FAR
model7_max <- train(max_price ~ . , data = maxPrice_Clean_Training,
method = "parRF", trControl = train.control, metric = "MAE")
##### Train the model 8 Stochastic Gradient Boosting # warning for some brands (few observations)
model8_max <- train(max_price ~ . , data = maxPrice_Clean_Training,
method = "gbm", trControl = train.control, metric = "MAE")
#--------Models for min_price with Normalized data (except decision tree models) -----------------
#Features: brand, touchscreen, screen_size , weight, ram, storage, ssd, resolution, discrete_gpu,
#          cpu_benchmark_score, gpu_benchmark_score
##### Train the model 1 (Linear regression)
model1_min <- train(min_price ~ . , data = minPrice_Norm_Training,
method = "lm", trControl = train.control, metric = "MAE") #warning a lot of features
##### Train the model 2 (Generalized Linear Model without func specified -> could be improved)
model2_min <- train(min_price ~ . , data = minPrice_Norm_Training,
method = "glm", trControl = train.control, metric = "MAE") #warning a lot of features
##### Train the model 3 (GLM with Step AIC)
model3_min <- train(min_price ~ . , data = minPrice_Norm_Training,
method = "bayesglm", trControl = train.control, metric = "MAE")
##### Train the model 4 (Elastic net (glm))
model4_min <- train(min_price ~ . , data = minPrice_Norm_Training,
method = "glmnet", trControl = train.control, metric = "MAE")
##### Train the model 5 Boosted Tree
model5_min <- train(min_price ~ . , data = minPrice_Clean_Training,
method = "bstTree", trControl = train.control, metric = "MAE")
##### Train the model 6 eXtreme Gradient Boosting
model6_min <- train(min_price ~ . , data = minPrice_Clean_Training,
method = "xgbTree", trControl = train.control, metric = "MAE")
##### Train the model 7 Parallel Random Forest  <---------------BEST MODEL SO FAR
model7_min <- train(min_price ~ . , data = minPrice_Clean_Training,
method = "parRF", trControl = train.control, metric = "MAE")
##### Train the model 8 Stochastic Gradient Boosting # warning for some brands (few observations)
model8_min <- train(min_price ~ . , data = minPrice_Clean_Training,
method = "gbm", trControl = train.control, metric = "MAE")
#------- Summarize the results ----------------
#Features: brand, touchscreen, screen_size , weight, ram, storage, ssd, resolution, discrete_gpu,
#          cpu_benchmark_score, gpu_benchmark_score
print(model1_max$results$MAE+model1_min$results$MAE)
print(model2_max$results$MAE+model2_min$results$MAE)
print(model3_max$results$MAE+model3_min$results$MAE)
print(min(model4_max$results$MAE+model4_min$results$MAE))
print(min(model5_max$results$MAE+model5_min$results$MAE))
print(min(model6_max$results$MAE+model6_min$results$MAE))
print(min(model7_max$results$MAE+model7_min$results$MAE)) # <---------------BEST MODEL SO FAR
print(min(model8_max$results$MAE+model8_min$results$MAE))
maxPrice_Clean_Training <- clean6 %>%
select(brand_mean_max,base_name_mean_max,touchscreen,screen_surface,screen_size,weight,ram,storage,ssd,resolution_mean_max,discrete_gpu,gpu_mean_max,cpu_mean_max,display_mean_max,x360,os, max_price)
maxPrice_Clean_Training_fact <- data.frame(model.matrix(~., data=maxPrice_Clean_Training)) #Some models require dummy variables (factors) to be predefined (one-hot encoding). e.g Boosted Tree
minPrice_Clean_Training <- clean6 %>%
select(brand_mean_min,base_name_mean_min,touchscreen,screen_surface,screen_size,weight,ram,storage,ssd,resolution_mean_min,discrete_gpu,gpu_mean_min,cpu_mean_min,display_mean_min,x360,os, min_price)
minPrice_Clean_Training_fact <- data.frame(model.matrix(~., data=minPrice_Clean_Training))
# #-------- Data normalization -------------------
index_Response <- match(c("max_price", "min_price"), names(clean6))
preProcValues <- preProcess(clean6[-index_Response], method = "range")
trainScaled <- predict(preProcValues, clean6)
glimpse(trainScaled)
testScaled <- predict(preProcValues, clean_test3)
glimpse(testScaled)
# Selecting only the features to use for Normalized data
maxPrice_Norm_Training <- trainScaled %>%
select(brand_mean_max,base_name_mean_max,touchscreen,screen_surface,screen_size,weight,ram,storage,ssd,resolution_mean_max,discrete_gpu,gpu_mean_max,cpu_mean_max,display_mean_max,x360,os, max_price)
maxPrice_Norm_Training_fact <- data.frame(model.matrix(~., data=maxPrice_Norm_Training))
minPrice_Norm_Training <- trainScaled %>%
select(brand_mean_min,base_name_mean_min,touchscreen,screen_surface,screen_size,weight,ram,storage,ssd,resolution_mean_min,discrete_gpu,gpu_mean_min,cpu_mean_min,display_mean_min,x360,os, min_price)
minPrice_Norm_Training_fact <- data.frame(model.matrix(~., data=minPrice_Norm_Training))
#------Repeated K-Fold Cross Validation (K = 20, repeats = 3)----------------
# Training control definition
set.seed(123) #For reproducibility
train.control <- trainControl(method = "repeatedcv",
number = 20, repeats = 3)
##### Train the model 1 (Linear regression)
model1_max <- train(max_price ~ . , data = maxPrice_Norm_Training,
method = "lm", trControl = train.control, metric = "MAE") #warning a lot of features
##### Train the model 2 (Generalized Linear Model without func specified -> could be improved)
model2_max <- train(max_price ~ . , data = maxPrice_Norm_Training,
method = "glm", trControl = train.control, metric = "MAE") #warning a lot of features
##### Train the model 3 (Bayesian GLM)
model3_max <- train(max_price ~ . , data = maxPrice_Norm_Training,
method = "bayesglm", trControl = train.control, metric = "MAE")
##### Train the model 4 (Elastic net (glm))
model4_max <- train(max_price ~ . , data = maxPrice_Norm_Training,
method = "glmnet", trControl = train.control, metric = "MAE")
##### Train the model 6 eXtreme Gradient Boosting
model6_max <- train(max_price ~ . , data = maxPrice_Clean_Training,
method = "xgbTree", trControl = train.control, metric = "MAE")
##### Train the model 7 Parallel Random Forest  <---------------BEST MODEL SO FAR
model7_max <- train(max_price ~ . , data = maxPrice_Clean_Training,
method = "parRF", trControl = train.control, metric = "MAE")
##### Train the model 8 Stochastic Gradient Boosting # warning for some brands (few observations)
model8_max <- train(max_price ~ . , data = maxPrice_Clean_Training,
method = "gbm", trControl = train.control, metric = "MAE")
print(model1_max$results$MAE+model1_min$results$MAE)
print(model2_max$results$MAE+model2_min$results$MAE)
print(model3_max$results$MAE+model3_min$results$MAE)
print(min(model4_max$results$MAE+model4_min$results$MAE))
print(min(model5_max$results$MAE+model5_min$results$MAE))
print(min(model6_max$results$MAE+model6_min$results$MAE))
print(min(model7_max$results$MAE+model7_min$results$MAE)) # <---------------BEST MODEL SO FAR
print(min(model8_max$results$MAE+model8_min$results$MAE))
library("caretEnsemble")
set.seed(123) #For reproducibility
model_list <- caretList(max_price~., data=maxPrice_Clean_Training, trControl=train.control, methodList=c("parRF", "gbm"))
xyplot(resamples(model_list))
modelCor(resamples(model_list))
library(caretEnsemble)
library(caretEnsemble)
install.packages(caretEnsemble)
library(caretEnsemble)
install.packages(caretEnsemble)
install.packages("caretEnsemble")
library(caretEnsemble)
set.seed(123) #For reproducibility
model_list <- caretList(max_price~., data=maxPrice_Clean_Training, trControl=train.control, methodList=c("parRF", "gbm"))
xyplot(resamples(model_list))
modelCor(resamples(model_list))
set.seed(123) #For reproducibility
model_list <- caretList(max_price~., data=maxPrice_Clean_Training, trControl=train.control, methodList=c("parRF", "gbm", "glmnet"))
set.seed(123) #For reproducibility
model_list2 <- caretList(max_price~., data=maxPrice_Clean_Training_fact, trControl=train.control, methodList=c("parRF","bstTree", "gbm", "glmnet"))
xyplot(resamples(model_list))
modelCor(resamples(model_list))
set.seed(123) #For reproducibility
train.control <- trainControl(method = "repeatedcv", savePredictions = "final",
number = 20, repeats = 3)
set.seed(123) #For reproducibility
model_list <- caretList(max_price~., data=maxPrice_Clean_Training, trControl=train.control, methodList=c("parRF", "gbm", "glmnet", "bayesglm", "xgbTree", "lm"))
set.seed(123) #For reproducibility
model_list2 <- caretList(max_price~., data=maxPrice_Clean_Training_fact, trControl=train.control, methodList=c("parRF","bstTree", "gbm", "glmnet", "bayesglm", "xgbTree", "lm"))
xyplot(resamples(model_list))
modelCor(resamples(model_list))
modelCor(resamples(model_list2))
xyplot(resamples(model_list[1],model_list[3]))
xyplot(resamples(model_list[1,3]))
model_list
xyplot(resamples(model_list$parRF, model_list$glmnet))
names(model_list)
model_list$glmnet
xyplot(resamples(model_list), models = c("parRF", "glmnet"), metric = "MAE")
set.seed(123) #For reproducibility
model_list_max <- caretList(max_price~., data=maxPrice_Clean_Training, trControl=train.control, methodList=c("parRF", "gbm", "glmnet"))
set.seed(123) #For reproducibility
model_list_min <- caretList(min_price~., data=minPrice_Clean_Training, trControl=train.control, methodList=c("parRF", "gbm", "glmnet"))
xyplot(resamples(model_list_max), models = c("parRF", "gbm"), metric = "MAE")
xyplot(resamples(model_list_max), models = c("parRF", "glmnet"), metric = "MAE")
xyplot(resamples(model_list_max), models = c("gbm", "glmnet"), metric = "MAE")
xyplot(resamples(model_list_min), models = c("parRF", "gbm"), metric = "MAE")
xyplot(resamples(model_list_min), models = c("parRF", "glmnet"), metric = "MAE")
xyplot(resamples(model_list_min), models = c("gbm", "glmnet"), metric = "MAE")
modelCor(resamples(model_list_max))
modelCor(resamples(model_list_min))
gbm_ensemble_max <- caretStack(model_list_max, method="gbm", metric="MAE", trControl=train.control)
gbm_ensemble_min <- caretStack(model_list_min, method="gbm", metric="MAE", trControl=train.control)
glm_ensemble_max <- caretStack(model_list_max, method="glmnet", metric="MAE", trControl=train.control)
glm_ensemble_min <- caretStack(model_list_min, method="glmnet", metric="MAE", trControl=train.control)
lm_ensemble_max <- caretStack(model_list_max, method="lm", metric="MAE", trControl=train.control)
lm_ensemble_min <- caretStack(model_list_min, method="lm", metric="MAE", trControl=train.control)
names(lm_ensemble_max)
names(lm_ensemble_max$error)
gbm_ensemble_max$error$MAE+gbm_ensemble_min$error$MAE
glm_ensemble_max$error$MAE+glm_ensemble_min$error$MAE
lm_ensemble_max$error$MAE+lm_ensemble_min$error$MAE
min(gbm_ensemble_max$error$MAE+gbm_ensemble_min$error$MAE)
min(glm_ensemble_max$error$MAE+glm_ensemble_min$error$MAE)
lm_ensemble_max$error$MAE+lm_ensemble_min$error$MAE
Price_Test_max <- clean_test3 %>%
select(brand_mean_max,base_name_mean_max,touchscreen,screen_surface,screen_size,weight,ram,storage,ssd,resolution_mean_max,discrete_gpu,gpu_mean_max,cpu_mean_max,display_mean_max,x360,os)
Price_Test_min <- clean_test3 %>%
select(brand_mean_min,base_name_mean_min,touchscreen,screen_surface,screen_size,weight,ram,storage,ssd,resolution_mean_min,discrete_gpu,gpu_mean_min,cpu_mean_min,display_mean_min,x360,os)
missingcol <- names(maxPrice_Clean_Training[!(names(maxPrice_Clean_Training[, !(names(maxPrice_Clean_Training) == "max_price")]) %in% names(Price_Test))])
Price_Test[missingcol] <- 0
missingcol <- names(maxPrice_Clean_Training[!(names(maxPrice_Clean_Training[, !(names(maxPrice_Clean_Training) == "max_price")]) %in% names(Price_Test))])
Price_Test_max[missingcol] <- 0
Price_Test_min[missingcol] <- 0
missingcol <- names(maxPrice_Clean_Training[!(names(maxPrice_Clean_Training[, !(names(maxPrice_Clean_Training) == "max_price")]) %in% names(Price_Test_max))])
Price_Test_max[missingcol] <- 0
Price_Test_min[missingcol] <- 0
View(Price_Test_max)
id_test <- clean_test3 %>% select(id)
pred_max <- data.frame(predict(gbm_ensemble_max, Price_Test_max, type = "raw")) #Ensemble GBM
pred_min <- data.frame(predict(gbm_ensemble_min, Price_Test_min, type = "raw")) #Ensemble GBM
names(pred_max) <- "MAX"
names(pred_min) <- "MIN"
results <- cbind(id_test,pred_max, pred_min)
results
write.csv(results, file = "Model 10 (GBM ensemble).csv", row.names = F)
minPrice_Clean_Training2 <- clean6 %>%
select(brand_mean_max,base_name_mean_max,touchscreen,screen_surface,screen_size,weight,ram,storage,ssd,resolution_mean_max,discrete_gpu,gpu_mean_max,cpu_mean_max,display_mean_max,x360,os, min_price)
set.seed(123)
model9_min <- train(min_price ~ . , data = minPrice_Clean_Training2,
method = "parRF", trControl = train.control, metric = "MAE")
results <- cbind(id_test, pred_min, pred_max)
results
write.csv(results, file = "Model 11 (GBM ensemble_corretion).csv", row.names = F)
bothmodels <- c(model9_min,model7_max)
pred <- data.frame(predict(bothmodels, Price_Test_max, type = "raw"))
names(pred) <- c("MIN", "MAX")
results <- cbind(id_test, pred)
results
write.csv(results, file = "Model 12 (parRF mean_max).csv", row.names = F)
bothmodels <- list(model9_min,model7_max)
pred <- data.frame(predict(bothmodels, Price_Test_max, type = "raw"))
names(pred) <- c("MIN", "MAX")
results <- cbind(id_test, pred)
results
write.csv(results, file = "Model 12 (parRF mean_max).csv", row.names = F)
